{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6678dd",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea224b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "import os, numba\n",
    "import time\n",
    "from tensorflow.keras import mixed_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1723b1a2",
   "metadata": {},
   "source": [
    "# Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b050c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/Users/jc/Documents/Snl4_data/Data_train/ERA5/Data/'\n",
    "data_input = pd.read_csv(f'{base_path}input.csv', header=None) \n",
    "data_input_dia = pd.read_csv(f'{base_path}input_dia.csv', header=None)\n",
    "data_output = pd.read_csv(f'{base_path}output.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7198924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in0 = data_input.to_numpy()\n",
    "data_in_dia0 = data_input_dia.to_numpy()\n",
    "data_out0 = data_output.to_numpy()\n",
    "variables = {\n",
    "    'data_in': data_in0,\n",
    "    'data_in_dia': data_in_dia0,\n",
    "    'data_out': data_out0,\n",
    "}\n",
    "num_locations = 9\n",
    "num_years = 8\n",
    "year_points = [2928 if i in [0, 4] else 2920 for i in range(num_years)]  # Points per year\n",
    "total_points_per_location = sum(year_points)\n",
    "data_loc0 = np.concatenate([np.ones(total_points_per_location, dtype=int) * loc for loc in range(1, num_locations + 1)]).reshape(-1, 1)\n",
    "train_split = 0.9  \n",
    "def split_by_location(data):\n",
    "    train_data, test_data = [], []\n",
    "    for loc_idx in range(num_locations):\n",
    "        start_idx = loc_idx * total_points_per_location\n",
    "        end_idx = start_idx + total_points_per_location\n",
    "        loc_data = data[start_idx:end_idx, :]\n",
    "        trainn = int(len(loc_data) * train_split)\n",
    "        train_data.append(loc_data[:trainn, :])\n",
    "        test_data.append(loc_data[trainn:, :])\n",
    "    train_combined = np.concatenate(train_data, axis=0)\n",
    "    train_indices = np.random.RandomState(seed=42).permutation(len(train_combined))\n",
    "    train_combined = train_combined[train_indices] \n",
    "    return train_combined, np.concatenate(test_data, axis=0)\n",
    "for var_name, var_data in variables.items():\n",
    "    globals()[f\"{var_name}_train\"], globals()[f\"{var_name}_test\"] = split_by_location(var_data)\n",
    "\n",
    "def reshape_and_transpose(x):\n",
    "    return x.reshape(x.shape[0], 40, 24).transpose(0, 2, 1)[:, :, :40]\n",
    "data_in1_train     = reshape_and_transpose(data_in_train)\n",
    "data_in1_dia_train = reshape_and_transpose(data_in_dia_train)\n",
    "data_in1_dia_test  = reshape_and_transpose(data_in_dia_test)\n",
    "data_in1_test      = reshape_and_transpose(data_in_test)\n",
    "data_out1_train    = reshape_and_transpose(data_out_train)\n",
    "data_out1_test     = reshape_and_transpose(data_out_test)\n",
    "data_input_test      = reshape_and_transpose(data_input.to_numpy())\n",
    "data_input_dia_test  = reshape_and_transpose(data_input_dia.to_numpy())\n",
    "data_output_test     = reshape_and_transpose(data_output.to_numpy())\n",
    "\n",
    "print(f\"Training_size: {data_input_test.shape}\")\n",
    "print(f\"Validation_size: {data_out1_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e593cd14",
   "metadata": {},
   "source": [
    "## Spectrum input only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9c858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in1_trainsk = np.stack([data_in1_train], axis=-1) #normalized by max value\n",
    "data_in1_testsk = np.stack([data_in1_test], axis=-1) \n",
    "np.random.seed(42)\n",
    "random_indices = np.random.choice(data_in1_trainsk.shape[0], size=data_in1_trainsk.shape[0], replace=False)\n",
    "selected_data_in1x = data_in1_trainsk[random_indices, :, :40]\n",
    "selected_data_out1x = data_out1_train[random_indices, :, :40]\n",
    "spec_norm1 = np.std(selected_data_in1x)\n",
    "snl_norm1 = np.std(selected_data_out1x)\n",
    "selected_data_in1n = selected_data_in1x/spec_norm1\n",
    "selected_data_out1n = selected_data_out1x/snl_norm1\n",
    "x_train, x_vali, y_train, y_vali = train_test_split(selected_data_in1n, selected_data_out1n, test_size=0.2, shuffle=True)\n",
    "x_train = x_train.reshape((x_train.shape[0], 24, 40, 1))\n",
    "y_train = y_train.reshape((y_train.shape[0], 24, 40, 1))\n",
    "x_vali = x_vali.reshape((x_vali.shape[0], 24, 40, 1))\n",
    "y_vali = y_vali.reshape((y_vali.shape[0], 24, 40, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc13fb0d",
   "metadata": {},
   "source": [
    "## U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31576ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv_block(x, n_filters):\n",
    "    x = layers.Conv2D(n_filters, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(n_filters, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "def downsample_block(x, n_filters):\n",
    "    f = double_conv_block(x, n_filters)\n",
    "    p = layers.MaxPool2D(2)(f)\n",
    "    return f, p\n",
    "\n",
    "def upsample_block(x, conv_features, n_filters):\n",
    "    x = layers.Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(x)\n",
    "    x = layers.concatenate([x, conv_features])\n",
    "    x = double_conv_block(x, n_filters)\n",
    "    return x\n",
    "\n",
    "def build_unet_model():\n",
    "    inputs = layers.Input(shape=(24, 40, 1))\n",
    "    f1, p1 = downsample_block(inputs, 32)\n",
    "    f2, p2 = downsample_block(p1, 64)\n",
    "    bottleneck = double_conv_block(p2, 128)\n",
    "    u6 = upsample_block(bottleneck, f2, 64)\n",
    "    u7 = upsample_block(u6, f1, 32)\n",
    "    outputs = layers.Conv2D(1, 1, padding=\"same\", activation=\"linear\")(u7)\n",
    "    model1 = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n",
    "    return model1\n",
    "\n",
    "model1 = build_unet_model()\n",
    "model1.compile(optimizer=tf.keras.optimizers.Adam(), loss=\"mse\")\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history1 = model1.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_vali, y_vali),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbeba44",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec278cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block(x, n_units):\n",
    "    x = layers.Dense(n_units, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def build_feedforward_model():\n",
    "    inputs = layers.Input(shape=(24, 40, 1))\n",
    "\n",
    "    x = layers.Flatten()(inputs)\n",
    "\n",
    "    x = dense_block(x, 80)\n",
    "    x = dense_block(x, 39)\n",
    "    x = dense_block(x, 80)\n",
    "\n",
    "    x = layers.Dense(24*40, activation=\"linear\")(x)\n",
    "\n",
    "    outputs = layers.Reshape((24, 40, 1))(x)\n",
    "    model3 = tf.keras.Model(inputs, outputs, name=\"Feedforward-NN\")\n",
    "    return model3\n",
    "\n",
    "model3 = build_feedforward_model()\n",
    "model3.compile(optimizer=tf.keras.optimizers.Adam(), loss=\"mse\")\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history1 = model3.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_vali, y_vali),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e1ffc1",
   "metadata": {},
   "source": [
    "## DIA as an additional input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e44edf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in1_trainsk = np.stack([data_in1_train, (data_in1_dia_train)], axis=-1)\n",
    "data_in1_testsk = np.stack([data_in1_test, (data_in1_dia_test)], axis=-1)\n",
    "np.random.seed(42)\n",
    "random_indices = np.random.choice(data_in1_trainsk.shape[0], size=data_in1_trainsk.shape[0], replace=False)\n",
    "selected_data_in1x = data_in1_trainsk[random_indices, :, :40]\n",
    "selected_data_out1x = data_out1_train[random_indices, :, :40]\n",
    "spec_norm1 = np.std(selected_data_in1x)\n",
    "snl_norm1 = np.std(selected_data_out1x)\n",
    "selected_data_in1n = selected_data_in1x/spec_norm1\n",
    "selected_data_out1n = selected_data_out1x/snl_norm1\n",
    "x_train, x_vali, y_train, y_vali = train_test_split(selected_data_in1n, selected_data_out1n, test_size=0.2, shuffle=True)\n",
    "x_train = x_train.reshape((x_train.shape[0], 24, 40, 1))\n",
    "y_train = y_train.reshape((y_train.shape[0], 24, 40, 1))\n",
    "x_vali = x_vali.reshape((x_vali.shape[0], 24, 40, 1))\n",
    "y_vali = y_vali.reshape((y_vali.shape[0], 24, 40, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c101ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv_block(x, n_filters):\n",
    "    x = layers.Conv2D(n_filters, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(n_filters, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "def downsample_block(x, n_filters):\n",
    "    f = double_conv_block(x, n_filters)\n",
    "    p = layers.MaxPool2D(2)(f)\n",
    "    return f, p\n",
    "\n",
    "def upsample_block(x, conv_features, n_filters):\n",
    "    x = layers.Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(x)\n",
    "    x = layers.concatenate([x, conv_features])\n",
    "    x = double_conv_block(x, n_filters)\n",
    "    return x\n",
    "\n",
    "def build_unet_model():\n",
    "    inputs = layers.Input(shape=(24, 40, 2))\n",
    "    f1, p1 = downsample_block(inputs, 32)\n",
    "    f2, p2 = downsample_block(p1, 64)\n",
    "    bottleneck = double_conv_block(p2, 128)\n",
    "    u6 = upsample_block(bottleneck, f2, 64)\n",
    "    u7 = upsample_block(u6, f1, 32)\n",
    "    outputs = layers.Conv2D(1, 1, padding=\"same\", activation=\"linear\")(u7)\n",
    "    model1 = tf.keras.Model(inputs, outputs, name=\"U-Net\")\n",
    "    return model1\n",
    "\n",
    "model1 = build_unet_model()\n",
    "model1.compile(optimizer=tf.keras.optimizers.Adam(), loss=\"mse\")\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 200\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "history1 = model1.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_vali, y_vali),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd82923d",
   "metadata": {},
   "source": [
    "## Quantile loss function (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a2d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_loss(y_true, y_pred, q):\n",
    "    e = y_true - y_pred\n",
    "    return tf.keras.backend.maximum(q * e, (q - 1) * e)\n",
    "\n",
    "def combined_mse_and_range_loss(y_true, y_pred, alpha=1.0, beta=1.0):\n",
    "    y_true = tf.squeeze(y_true, axis=-1)\n",
    "    mean = y_pred[..., 0]\n",
    "    variation = tf.math.abs(y_pred[..., 1])\n",
    "    mse_loss = tf.reduce_mean(tf.square(y_true - mean))\n",
    "\n",
    "    y_lower = mean - variation\n",
    "    y_upper = mean + variation\n",
    "    loss_low = quantile_loss(y_true, y_lower, q=0.025)\n",
    "    loss_up  = quantile_loss(y_true, y_upper, q=0.975)\n",
    "    range_loss = tf.reduce_mean(loss_low + loss_up)\n",
    "\n",
    "    return alpha * mse_loss + beta * range_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286fc781",
   "metadata": {},
   "source": [
    "# Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfbaac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/Users/jc/Documents/Snl4_data/Data_train/ERA5/Data/'\n",
    "def load_and_reshape(name):\n",
    "    arr = pd.read_csv(f'{base_path}{name}.csv', header=None).to_numpy()\n",
    "    return arr.reshape(arr.shape[0], 40, 24).transpose(0, 2, 1)[:, :, :40]\n",
    "data_input_test = load_and_reshape('input_test')\n",
    "data_input_dia_test = load_and_reshape('input_dia_test')\n",
    "data_output_test = load_and_reshape('output_test')\n",
    "print(f\"Test_size: {data_input_test.shape}\")\n",
    "\n",
    "data_input_testsk = data_input_test[..., None]\n",
    "pred = model1.predict(data_input_testsk / spec_norm1) * snl_norm1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
